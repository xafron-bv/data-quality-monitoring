# Data Quality Detection System

A comprehensive, multi-method data quality monitoring system for detecting errors and anomalies in structured data, with a focus on fashion/retail product catalogs.

## Overview

This system combines multiple detection approaches to identify data quality issues:

- **Validation (Rule-Based)**: High-confidence error detection using business rules
- **Pattern-Based Anomaly Detection**: Medium-confidence detection using pattern matching
- **ML-Based Detection**: Semantic similarity analysis using sentence transformers
- **LLM-Based Detection**: Advanced semantic understanding with language models

The system is designed to be field-agnostic and brand-independent, making it adaptable to various data domains.

## Key Features

- ğŸ¯ **Multi-Method Detection**: Combines rule-based, pattern-based, and ML approaches
- ğŸ“Š **Comprehensive Evaluation**: Built-in metrics and confusion matrix analysis
- ğŸ”§ **Configurable**: Field mappings and detection thresholds are customizable
- ğŸ’¾ **Memory Efficient**: Sequential processing and model caching
- ğŸ“ˆ **Performance Optimization**: Weighted combination based on historical performance
- ğŸŒ **Brand Agnostic**: Supports multiple brands through configuration
- ğŸ“± **Visual Interface**: HTML5 viewer for interactive result exploration

## Installation

### Prerequisites

- Python 3.8 or higher
- CUDA-capable GPU (optional, for faster ML/LLM detection)
- 8GB+ RAM recommended

### Setup

1. Clone the repository:
```bash
git clone <repository-url>
cd detection-system
```

2. Create a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Configure brand settings:
```bash
# Edit brand_config.json with your field mappings
```

## Project Structure

The project is organized with a clear separation between entrypoints, dependencies, and data:

```
.
â”œâ”€â”€ multi_sample_evaluation.py      # Main evaluation entrypoint
â”œâ”€â”€ analyze_column.py               # Column analysis tool
â”œâ”€â”€ generate_detection_weights.py   # Weight generation for detection methods
â”œâ”€â”€ llm_model_training.py          # LLM model training entrypoint
â”œâ”€â”€ ml_index.py                    # ML model indexing and training
â”œâ”€â”€ common/                        # Shared modules and utilities
â”‚   â”œâ”€â”€ brand_config.py           # Brand configuration loader
â”‚   â”œâ”€â”€ common_interfaces.py      # Common interfaces and data structures
â”‚   â”œâ”€â”€ comprehensive_detector.py  # Main detection orchestrator
â”‚   â”œâ”€â”€ consolidated_reporter.py   # Unified reporting
â”‚   â”œâ”€â”€ debug_config.py           # Debug configuration
â”‚   â”œâ”€â”€ error_injection.py        # Error injection for testing
â”‚   â”œâ”€â”€ evaluator.py              # Evaluation framework
â”‚   â”œâ”€â”€ exceptions.py             # Custom exceptions
â”‚   â”œâ”€â”€ field_column_map.py       # Field to column mapping
â”‚   â”œâ”€â”€ field_mapper.py           # Field mapping utilities
â”‚   â”œâ”€â”€ ml_curve_generator.py     # ML performance curve generation
â”‚   â””â”€â”€ unified_detection_interface.py  # Unified detection interface
â”œâ”€â”€ anomaly_detectors/             # Anomaly detection modules
â”‚   â”œâ”€â”€ ml_based/                 # ML-based detection
â”‚   â”œâ”€â”€ pattern_based/            # Pattern-based detection
â”‚   â””â”€â”€ llm_based/                # LLM-based detection
â”œâ”€â”€ validators/                    # Business rule validators
â”œâ”€â”€ brand_configs/                 # Brand-specific configurations
â””â”€â”€ data/                         # Data directory (git-ignored)
```

### Report Directories

Each entrypoint script generates reports in its own subdirectory:
- `multi_sample_evaluation/reports/` - Evaluation results and metrics
- `analyze_column/reports/` - Column analysis results
- `generate_detection_weights/reports/` - Detection weight configurations
- `llm_model_training/reports/` - LLM training results
- `ml_index/reports/` - ML model training results

Each entrypoint directory contains a `viewer.html` file for browsing reports in its `reports/` subdirectory.

## Quick Start

### Running the Demo

The easiest way to see the system in action:

```bash
# Basic demo with all detection methods
python single_sample_multi_field_demo.py \
    --data-file your_data.csv \
    --enable-validation \
    --enable-pattern \
    --enable-ml
```

### Basic Usage

1. **Analyze your data**:
```bash
python analyze_column.py --data-file your_data.csv --column product_name
```

2. **Run detection**:
```bash
python single_sample_multi_field_demo.py \
    --data-file your_data.csv \
    --injection-intensity 0.2 \
    --output-dir results
```

3. **View results**:
- Open `data_quality_viewer.html` in your browser
- Upload the generated CSV and JSON files from the results directory

## Configuration

### Brand Configuration

Edit `brand_config.json` to map your data columns to standard fields:

```json
{
    "brand_name": "your_brand",
    "field_mappings": {
        "material": "Material_Column",
        "color_name": "Color_Description",
        "category": "Product_Category"
    },
    "default_data_path": "data/your_data.csv"
}
```

### Detection Thresholds

Adjust detection sensitivity:

```bash
python single_sample_multi_field_demo.py \
    --validation-threshold 0.0 \
    --anomaly-threshold 0.7 \
    --ml-threshold 0.7 \
    --llm-threshold 0.6
```

## Adding New Fields

### 1. Rule-Based Validation

Create a new validator:
```bash
mkdir validators/new_field
# Create validators/new_field/validate.py
# Create validators/new_field/error_messages.json
```

### 2. Pattern-Based Detection

Add pattern rules:
```bash
# Create anomaly_detectors/pattern_based/rules/new_field.json
```

### 3. ML-Based Detection

Train a model:
```bash
# Add field config in model_training.py
python anomaly_detectors/ml_based/model_training.py --field new_field
```

## Advanced Usage

### Performance Optimization

Generate optimized weights based on evaluation results:
```bash
python generate_detection_weights.py \
    -i results/report.json \
    -o detection_weights.json
```

Use weighted combination:
```bash
python single_sample_multi_field_demo.py \
    --use-weighted-combination \
    --weights-file detection_weights.json
```

### Batch Evaluation

For systematic performance evaluation:
```bash
python multi_sample_evaluation.py \
    --data-file your_data.csv \
    --num-samples 10 \
    --sample-size 1000
```

### ML Model Analysis

Analyze ML model performance:
```bash
python ml_curve_generator.py \
    --field material \
    --output-dir ml_analysis
```

## Architecture

The system follows a modular, layered architecture:

- **Entry Points**: User-facing scripts for different use cases
- **Orchestration**: Coordinates detection methods and manages workflow
- **Detection Methods**: Independent implementations of each detection approach
- **Core Services**: Shared utilities for configuration, mapping, and reporting
- **Data Layer**: Handles data I/O and storage

## Performance Considerations

- **Memory Usage**: The system processes fields sequentially to minimize memory footprint
- **GPU Acceleration**: ML and LLM detection can utilize GPU if available
- **Caching**: Models are cached to avoid redundant loading
- **Batch Processing**: Configurable batch sizes for optimal performance

## Troubleshooting

### Common Issues

1. **Out of Memory**: 
   - Use `--core-fields-only` flag
   - Reduce batch size
   - Disable ML/LLM detection

2. **Slow Performance**:
   - Enable GPU acceleration
   - Use parallel processing
   - Optimize detection thresholds

3. **Missing Fields**:
   - Check field mappings in brand_config.json
   - Verify column names in your data

## Contributing

1. Follow the existing code structure
2. Add tests for new functionality
3. Update documentation
4. Submit pull requests with clear descriptions

## License

[Specify your license here]

## Support

For issues, questions, or contributions, please [specify contact method].